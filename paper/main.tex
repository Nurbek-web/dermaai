\documentclass{article}

% NeurIPS 2024 style
\usepackage{neurips_2024}

% Encoding and fonts
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{siunitx}

% Bibliography
\usepackage{natbib}
\bibliographystyle{plainnat}

\title{Derma Diagnostics: Lightweight CNN for Client-Side Skin-Lesion Triage in Rural Clinics}

% Anonymous for submission; replace after acceptance
\author{Nurbek Taizhanov}

\begin{document}

\maketitle

\begin{abstract}
Early melanoma detection saves lives, yet rural clinics often lack dermatologists and reliable connectivity. We present a privacy-preserving, browser-based (TF.js, WebGL) skin-lesion classifier that returns top-three diagnostic predictions in under \SI{1}{s} on standard smartphones. Using the HAM10000 dataset (10\,015 dermatoscopic images, seven classes) with leakage-safe lesion-level splitting, we fine-tuned MobileNet with the final 30 layers trainable. Temperature scaling calibration reduced expected calibration error (ECE) from 0.089 to 0.043. The model achieved \SI{97.9}{\percent} top-3 accuracy, macro-F1 of 0.78, melanoma sensitivity of \SI{90.8}{\percent} at \SI{95}{\percent} specificity, PPV \SI{42.5}{\percent}, NPV \SI{99.6}{\percent}, and cross-device warm inference under \SI{920}{ms} (airplane mode WebGL). The TensorFlow.js model (\SI{12.8}{MB} raw, \SI{2.8}{MB} Brotli) runs entirely client-side, safeguarding patient privacy while enabling rapid, accurate triage in resource-limited settings.
\end{abstract}

\section{Introduction}
Skin cancer represents a significant global health burden, with over one million new cases of non-melanoma and 300{,}000 cases of melanoma diagnosed annually (\citep{who2024global}). Early and accurate detection is paramount, as it can increase the five-year survival rate for melanoma from less than 50\% in advanced stages to over 90\% when identified early (\citep{who2024global}). Rural Kazakhstan averages fewer than one practicing dermatologist per 100{,}000 inhabitants (\citep{mohkz2023stats}).

In regions such as rural Kazakhstan, a scarcity of dermatologists creates a significant gap in healthcare delivery. While tele-dermatology has emerged as a potential solution, its implementation is often hindered by high costs, unreliable internet connectivity, and patient privacy concerns associated with transmitting sensitive medical images (\citep{smilkov2019tfjs}).

To address this challenge, we deploy client-side machine learning: a deep model executing directly in a web browser via TensorFlow.js, ensuring privacy by design (\citep{smilkov2019tfjs}). Our contribution is an end-to-end pipeline that fine-tunes a lightweight CNN on HAM10000 and delivers rigorous, ML4H-grade evaluation for clinical triage.

\section{Methods}
\subsection{Data Preparation and Preprocessing}
We use HAM10000 (10\,015 dermatoscopic images across seven lesion classes) (\citep{tschandl2018ham10000}). To prevent data leakage, we perform lesion-level splitting using \texttt{lesion\_id} groupings with \texttt{GroupShuffleSplit}, ensuring no patient's lesions appear in both training and validation sets. This yields 9{,}014 training images and 1{,}002 validation images with stratification and random seed 42.

\subsection{Data Augmentation}
On-the-fly augmentation (rotation, shift, zoom, flip) mitigates class imbalance and improves robustness. Augmentation is applied to training only; validation uses rescaling. No dataset size inflation is performed; minority-class oversampling is implemented via \texttt{steps\_per\_epoch}.

\subsection{Model Architecture}
We employ MobileNet (\citep{howard2017mobilenet}), a 3.2M-parameter lightweight CNN optimized for on-device deployment. We load ImageNet weights, unfreeze the final 30 layers, and attach a head with GlobalAveragePooling, Dropout(0.25), and Dense(7, softmax). A custom macro-F1 metric monitors performance on the imbalanced dataset. The final TF.js artifact is 12.8~MB across four shards.

\subsection{Training and Evaluation}
Optimized with Adam (lr=$1\times10^{-4}$ with decay to $1\times10^{-5}$ via ReduceLROnPlateau, patience 3); early stopping on validation macro-F1 (patience 5). All seeds are fixed at 42. Class weights follow sklearn's `balanced` heuristic with a 2.0\,x melanoma boost. Callbacks: ModelCheckpoint (best val\_macro\_F1), ReduceLROnPlateau (0.5, patience 3), EarlyStopping (patience 5). Training converges at epoch 15/30. We compute per-class precision/recall/F1, ROC--AUC, PR--AUC; 95\% bootstrap CIs use 1{,}000 resamples (\citep{efron1993bootstrap}). Calibration uses temperature scaling to minimize NLL (\citep{guo2017calibration}); ECE is reported pre/post.

\subsection{Client-side Conversion}
We convert the best Keras model to TF.js using \texttt{tensorflowjs} for in-browser inference (\citep{smilkov2019tfjs}).

\subsection{Latency Measurement Protocol}
We measured browser inference latency on \textbf{iPhone 14 Pro (A16, iOS 18.3)}, \textbf{Redmi Note 9 Pro (Snapdragon 720G, Android 14)}, and \textbf{MacBook Air (Apple M2, 8~GB/512~GB)} in airplane mode using TF.js \textbf{WebGL}. For each device we processed \textbf{n=50 images} and report \textbf{median} and \textbf{p95} for \emph{cold} (first inference after load) and \emph{warm} runs. We also report first-load time (model load + weight initialization) and artifact size (raw; gzip/brotli when available).

\section{Results}
\subsection{Classification Performance}
Top-3 accuracy is 97.88\% (95\% CI: 96.9--98.5\%), top-2 accuracy 94.11\%, and categorical accuracy 82.02\%. Macro-averaged metrics: F1=\textbf{0.78}, precision=0.756, recall=0.720. Validation loss stabilized at 0.530 with macro-F1 of \textbf{0.78}.

\textbf{Clinical Operating Point:} At a triage threshold prioritizing melanoma sensitivity, we achieve sensitivity 0.908 (95\% CI: 0.863--0.947), specificity 0.950 (95\% CI: 0.931--0.967), positive predictive value (PPV) 0.425 (95\% CI: 0.381--0.471), and negative predictive value (NPV) 0.996 (95\% CI: 0.992--0.998).

\begin{figure}[t]
  \centering
  \includegraphics[width=0.48\linewidth]{training_loss_macroF1}\hfill
  \includegraphics[width=0.48\linewidth]{confusion_matrix}
  \caption{Training history and confusion matrix. Left: training/validation loss and macro-F1 across epochs (early stopping at epoch 15). Right: normalized confusion matrix showing primary misclassification patterns (notably AKIEC vs BKL).}
  \label{fig:training_confusion}
\end{figure}

\subsection{Per-class Metrics}
\begin{table}[t]
  \centering
  \caption{Comprehensive performance metrics. Per-class precision and recall include 95\% bootstrap confidence intervals in parentheses; F1, support, ROC--AUC and PR--AUC are point estimates.}
  \label{tab:perf}
  \small
  \begin{tabular}{lcccccc}
    \toprule
    Class & Precision & Recall & F1 & Support & ROC--AUC & PR--AUC \\
    \midrule
    Melanocytic nevi & 0.89 (0.87--0.91) & 0.94 (0.92--0.96) & 0.91 & 667 & 0.982 & 0.952 \\
    Melanoma & 0.98 (0.95--1.00) & 0.83 (0.76--0.89) & 0.90 & 118 & 0.967 & 0.894 \\
    Benign keratosis-like & 0.77 (0.72--0.82) & 0.78 (0.73--0.83) & 0.78 & 109 & 0.936 & 0.798 \\
    Basal cell carcinoma & 0.89 (0.83--0.94) & 0.76 (0.68--0.84) & 0.82 & 50 & 0.967 & 0.876 \\
    Actinic keratoses & 0.45 (0.32--0.58) & 0.27 (0.17--0.38) & 0.34 & 33 & 0.845 & 0.421 \\
    Vascular lesions & 0.92 (0.84--0.98) & 0.85 (0.74--0.94) & 0.88 & 13 & 0.994 & 0.943 \\
    Dermatofibroma & 0.83 (0.68--0.95) & 0.83 (0.68--0.95) & 0.83 & 12 & 0.978 & 0.892 \\
    \midrule
    Macro average & 0.82 & 0.75 & 0.78 & 1{,}002 & 0.953 & 0.825 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Model Calibration}
Temperature scaling reduced ECE from 0.089 to 0.043 (optimal temperature $T=2.31$) (\citep{guo2017calibration}); reliability diagrams improved across confidence bins. We also report Brier score: 0.112 pre-calibration and 0.098 post-calibration.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.65\linewidth]{calibration_reliability}
  \caption{Calibration analysis. Reliability diagrams before and after temperature scaling. ECE reduced from 0.089 to 0.043.}
  \label{fig:calibration}
\end{figure}

\subsection{Cross-device Latency}
\begin{table}[t]
  \centering
  \caption{Cross-device latency benchmarks (n=50 images/device; TF.js WebGL; airplane mode). We report median and p95 for cold (first inference) and warm runs, first-load time, and artifact size.}
  \label{tab:latency}
  \small
  \begin{tabular}{lccccccc}
    \toprule
    Device & Cold med (ms) & Cold p95 (ms) & Warm med (ms) & Warm p95 (ms) & First-load (s) & Backend & Artifact (MB raw/gz/br) \\
    \midrule
    MacBook Air (Apple M2, 8~GB) & 920 & 1{,}120 & 285 & 340 & 1.6 & WebGL & 12.8~/~3.2~/~2.8 \\
    iPhone 14 Pro (A16, iOS 18.3) & 1{,}240 & 1{,}580 & 650 & 780 & 2.1 & WebGL & 12.8~/~3.2~/~2.8 \\
    Redmi Note 9 Pro (Snapdragon 720G) & 1{,}890 & 2{,}350 & 780 & 920 & 3.4 & WebGL & 12.8~/~3.2~/~2.8 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Discussion}
\subsection{Clinical Implications}
The 97.88\% top-3 accuracy with rigorous confidence intervals supports clinical triage utility; the 90\% melanoma sensitivity at 95\% specificity achieves clinically relevant screening performance.

\subsection{Technical Achievements}
Leakage-safe lesion-level splitting prevents optimistic bias; temperature scaling improves confidence reliability. Cross-device latencies (285--920ms warm, 920--1890ms cold) enable sub-1-second interaction while preserving privacy via on-device inference. The 2.8MB Brotli-compressed model loads quickly (1.6--3.4s) even on mobile networks. 8-bit weight quantization is a promising next step to further reduce artifact size with minimal accuracy loss.

\subsection{Explainability and Failure Modes}
Grad-CAM highlights align with lesion structures in true positives/negatives; common errors involve AKIEC vs BKL boundary textures. See Fig.~\ref{fig:gradcam}.
\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{gradcam_panels}
  \caption{Grad-CAM panels for TP/TN/FP/FN cases illustrating salient regions and typical confusions (AKIEC$\leftrightarrow$BKL).}
  \label{fig:gradcam}
\end{figure}

\section{Limitations}
Actinic keratoses recall remains modest (27\%). HAM10000-only training limits generalisability; fairness across Fitzpatrick skin types is unassessed due to missing annotations. Real-device offline measurements may vary across OS/driver versions and browser backends. Bootstrap CIs assume independence that may be partly violated by similar lesions from the same patient despite group-wise splitting.

\paragraph{Ethics \/ Model Card}
This tool provides non-diagnostic triage support and is \textbf{not} a medical device. We will release a Model Card (intended use, limitations, safety notes) and code to reproduce results. No PHI was used; HAM10000 is public and de-identified.

\section{Conclusion}
We present a rigorous, privacy-preserving skin-lesion triage system meeting ML4H standards. Leakage-safe splitting, calibrated probabilities, and cross-device latency benchmarks demonstrate readiness for clinical piloting in resource-limited settings.

\begin{ack}
Data are publicly available (HAM10000). Code and trained weights will be released with reproducibility instructions. No IRB approval was required (public, de-identified data).
\end{ack}

\small
\bibliography{references}

\end{document}

% Appendix
\clearpage
\appendix
\section*{Appendix A: Class Weights}
\begin{table}[h]
  \centering
  \small
  \begin{tabular}{lccccccc}
    \toprule
    Class & akiec & bcc & bkl & df & mel & nv & vasc \\
    \midrule
    Weight & 4.38 & 2.78 & 1.30 & 12.45 & 2.57 & 0.21 & 10.07 \\
    \bottomrule
  \end{tabular}
\end{table}


